{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAJCMQ4R22HB"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/StadynR/HAR-imu-photogrammetry/blob/main/Notebooks/Dataset%201/IMU%20-%20Best%20Features.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL5s36gHyqIN"
      },
      "source": [
        "# Dataset 1 - IMU (Best Features)\n",
        "\n",
        "This notebook contains code and explanations for the training and testing of 7 different AI architectures for the task of Human Action Recognition using time series data obtained from motion sensors (accelerometers, gyroscopes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8jX8ZM6p7eI"
      },
      "source": [
        "## Access to Google Drive and load dataset\n",
        "\n",
        "You need to create a shortcut in your Drive home to this folder: https://drive.google.com/drive/folders/1k2sAkmRyyctE1uOc19mrixyt2N47-7pt?usp=share_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "-KH1DO0fy53b",
        "outputId": "5ed49d37-cd1a-4113-8c9e-e5bc4c3d7a88"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-17deaa040e1f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Drive mount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#Drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8PQogp0yltI"
      },
      "outputs": [],
      "source": [
        "#Read document\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/Datasets/Dataset 1/United IMU Movements.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2VoazdSqCS_"
      },
      "source": [
        "## Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYsCf4ev0_9m"
      },
      "outputs": [],
      "source": [
        "# Data information (rows and columns)\n",
        "df.info()\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtayunmg1dX1"
      },
      "outputs": [],
      "source": [
        "# Count of instances of every activity\n",
        "countOfActivities = df['Movimiento'].value_counts()\n",
        "countOfActivities.plot(kind='bar',title='Number of readings by Activity Type',figsize=(14,8),grid=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOVAqlZq10Is"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "labs = [\"AGx\",'AGy','AGz', 'AAx','AAy','AAz', 'AMx','AMy','AMz','FAx','FAy','FAz','FGx','FGy','FGz','FMx','FMy','FMz','HAx','HAy','HAz','HGx','HGy','HGz','HMx','HMy','HMz']\n",
        "\n",
        "def plot_activity(activity,df,start=0,stop=20):\n",
        "  # extractRowsOfActivity = (df['Movimiento'] == activity)  # Output will be true/false.\n",
        "  data = df[:] # data has only rows that are for the requested activity.\n",
        "  # data = data[['AAx','AAy','AAz','AGx','AGy','AGz','AMx','AMy','AMz','FAx','FAy','FAz','FGx','FGy','FGz','FMx','FMy','FMz','HAx','HAy','HAz','HGx','HGy','HGz','HMx','HMy','HMz']] # data has only the accelerometer columns of one IMU.\n",
        "  data = data[['AAx','AAy','AAz','AGx','AGy','AGz','AMx','AMy','AMz','FAx','FAy','FAz','FGx','FGy','FGz']] # data has only the accelerometer columns of one IMU.\n",
        "  data = data[start:stop]\n",
        "\n",
        "  ax = data.plot(subplots=True,figsize=(16,12)) # Plot accelerometer for the activity.\n",
        "  ax = ax.flat\n",
        "  fig = ax[0].get_figure()\n",
        "  for idx, x in enumerate(ax):\n",
        "    x.legend([labs[idx]], loc=\"upper left\")\n",
        "    x.set_yticks([])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ_fSlPyqVyU"
      },
      "source": [
        "### Dataset information plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNzof2ls3Z5_"
      },
      "outputs": [],
      "source": [
        "plot_activity('Extension',df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClgAHo_58Dmb"
      },
      "outputs": [],
      "source": [
        "plot_activity('Flexion',df,100,500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soAFuWBl4gQc"
      },
      "source": [
        "### Feature Importance\n",
        "\n",
        "**Method:** Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV3zf5g14gQd"
      },
      "outputs": [],
      "source": [
        "df.iloc[:, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVUwU19V4gQd"
      },
      "outputs": [],
      "source": [
        "df.iloc[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fqTN7o4gQe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, X_test, Y, Y_test = train_test_split(df.iloc[:, 2:], df.iloc[:, 0], test_size=.2,random_state=4)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=.2,random_state=4)\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dqJwQF84gQe"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit the model to the data\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "def list_importances(features, importances):\n",
        "  zipped = zip(features, importances)\n",
        "  df = pd.DataFrame(list(sorted(zipped, reverse=True, key = lambda x: x[1])), columns =['Features', 'Importance'])\n",
        "  return df\n",
        "\n",
        "features = list(df.columns)[2:]\n",
        "df_importance = list_importances(features, clf.feature_importances_)\n",
        "df_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mol3Zm0qbnq"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "- Normalizing the measurements to be between 0 and 1.\n",
        "- Transforming the measurements to be in a 3-D array of [samples, timesteps,features].\n",
        "- Hot encoding the activity names.\n",
        "- Breaking the 3-D array into a training and test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ukz8gYsgdW"
      },
      "source": [
        "### Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o3VGvSY4Lya"
      },
      "outputs": [],
      "source": [
        "#NORMALIZATION\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()\n",
        "df['AAx_mms'] = mms.fit_transform(df[['AAx']])\n",
        "df['AAy_mms'] = mms.fit_transform(df[['AAy']])\n",
        "df['AAz_mms'] = mms.fit_transform(df[['AAz']])\n",
        "\n",
        "df['AGx_mms'] = mms.fit_transform(df[['AGx']])\n",
        "df['AGy_mms'] = mms.fit_transform(df[['AGy']])\n",
        "df['AGz_mms'] = mms.fit_transform(df[['AGz']])\n",
        "\n",
        "df['AMx_mms'] = mms.fit_transform(df[['AMx']])\n",
        "df['AMy_mms'] = mms.fit_transform(df[['AMy']])\n",
        "df['AMz_mms'] = mms.fit_transform(df[['AMz']])\n",
        "\n",
        "df['FAx_mms'] = mms.fit_transform(df[['FAx']])\n",
        "df['FAy_mms'] = mms.fit_transform(df[['FAy']])\n",
        "df['FAz_mms'] = mms.fit_transform(df[['FAz']])\n",
        "\n",
        "df['FGx_mms'] = mms.fit_transform(df[['FGx']])\n",
        "df['FGy_mms'] = mms.fit_transform(df[['FGy']])\n",
        "df['FGz_mms'] = mms.fit_transform(df[['FGz']])\n",
        "\n",
        "df['FMx_mms'] = mms.fit_transform(df[['FMx']])\n",
        "df['FMy_mms'] = mms.fit_transform(df[['FMy']])\n",
        "df['FMz_mms'] = mms.fit_transform(df[['FMz']])\n",
        "\n",
        "df['HAx_mms'] = mms.fit_transform(df[['HAx']])\n",
        "df['HAy_mms'] = mms.fit_transform(df[['HAy']])\n",
        "df['HAz_mms'] = mms.fit_transform(df[['HAz']])\n",
        "\n",
        "df['HGx_mms'] = mms.fit_transform(df[['HGx']])\n",
        "df['HGy_mms'] = mms.fit_transform(df[['HGy']])\n",
        "df['HGz_mms'] = mms.fit_transform(df[['HGz']])\n",
        "\n",
        "df['HMx_mms'] = mms.fit_transform(df[['HMx']])\n",
        "df['HMy_mms'] = mms.fit_transform(df[['HMy']])\n",
        "df['HMz_mms'] = mms.fit_transform(df[['HMz']])\n",
        "\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5biTcP4c3pQ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFcagiUgiRlV"
      },
      "outputs": [],
      "source": [
        "def plot_activity_norm(activity,df,start=0,stop=20):\n",
        "  extractRowsOfActivity = (df['Movimiento'] == activity)  # Output will be true/false.\n",
        "  data = df[extractRowsOfActivity] # data has only rows that are for the requested activity.\n",
        "  data = data[['Ax1_mms','Ay1_mms','Az1_mms','Gx1_mms','Gy1_mms','Gz1_mms', 'Mx1_mms','My1_mms','Mz1_mms','Ax2_mms','Ay2_mms','Az2_mms','Gx2_mms','Gy2_mms','Gz2_mms', 'Mx2_mms','My2_mms','Mz2_mms', 'Ax3_mms','Ay3_mms','Az3_mms','Gx3_mms','Gy3_mms','Gz3_mms', 'Mx3_mms','My3_mms','Mz3_mms']] # data has only the accelerometer columns of one IMU.\n",
        "  data = data[start:stop]\n",
        "\n",
        "  ax = data.plot(subplots=True,figsize=(16,12),title=activity+ ', Start Row: '+str(start)+' Stop row: '+str(stop)) # Plot accelerometer for the activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WEJL-1MhEAA"
      },
      "outputs": [],
      "source": [
        "plot_activity_norm('Extension',df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1gsnIDuhEAA"
      },
      "outputs": [],
      "source": [
        "plot_activity_norm('Flexion',df,100,500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnWx0M5js87G"
      },
      "source": [
        "### Transformation of dimensions as a 3D array\n",
        "\n",
        "Due to the low number of elements in the dataset, the number of samples was changed to 10, so that the final training and testing datasets do not have very few elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V__LcXyGt4T"
      },
      "outputs": [],
      "source": [
        "# Az3\t0.127717\n",
        "# Mx3\t0.127032\n",
        "# Mz3\t0.126262\n",
        "# Ay3\t0.113951"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oXw-Uie9FZm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "#nSamplesInEach = 200\n",
        "nSamplesInEach = 10\n",
        "nFeatures = 4\n",
        "samples = []\n",
        "labels = []\n",
        "for i in range(0,len(df)-nSamplesInEach,nSamplesInEach):  # Collecting samples\n",
        "  # ax1 = df['AAx_mms'].values[i:i+nSamplesInEach]\n",
        "  # ay1 = df['AAy_mms'].values[i:i+nSamplesInEach]\n",
        "  # az1 = df['AAz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # gx1 = df['AGx_mms'].values[i:i+nSamplesInEach]\n",
        "  # gy1 = df['AGy_mms'].values[i:i+nSamplesInEach]\n",
        "  # gz1 = df['AGz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # mx1 = df['AMx_mms'].values[i:i+nSamplesInEach]\n",
        "  # my1 = df['AMy_mms'].values[i:i+nSamplesInEach]\n",
        "  # mz1 = df['AMz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # ax2 = df['FAx_mms'].values[i:i+nSamplesInEach]\n",
        "  # ay2 = df['FAy_mms'].values[i:i+nSamplesInEach]\n",
        "  # az2 = df['FAz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # gx2 = df['FGx_mms'].values[i:i+nSamplesInEach]\n",
        "  # gy2 = df['FGy_mms'].values[i:i+nSamplesInEach]\n",
        "  # gz2 = df['FGz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # mx2 = df['FMx_mms'].values[i:i+nSamplesInEach]\n",
        "  # my2 = df['FMy_mms'].values[i:i+nSamplesInEach]\n",
        "  # mz2 = df['FMz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # ax3 = df['HAx_mms'].values[i:i+nSamplesInEach]\n",
        "  ay3 = df['HAy_mms'].values[i:i+nSamplesInEach]\n",
        "  az3 = df['HAz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  # gx3 = df['HGx_mms'].values[i:i+nSamplesInEach]\n",
        "  # gy3 = df['HGy_mms'].values[i:i+nSamplesInEach]\n",
        "  # gz3 = df['HGz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  mx3 = df['HMx_mms'].values[i:i+nSamplesInEach]\n",
        "  # my3 = df['HMy_mms'].values[i:i+nSamplesInEach]\n",
        "  mz3 = df['HMz_mms'].values[i:i+nSamplesInEach]\n",
        "\n",
        "  label = stats.mode(df['Movimiento'][i:i+nSamplesInEach])\n",
        "  label = label[0][0]\n",
        "\n",
        "  # samples.append([ax1, ay1, az1, gx1, gy1, gz1, mx1, my1, mz1, ax2, ay2, az2, gx2, gy2, gz2, mx2, my2, mz2, ax3, ay3, az3, gx3, gy3, gz3, mx3, my3, mz3])\n",
        "  samples.append([ay3, az3, mx3, mz3])\n",
        "  labels.append(label)\n",
        "\n",
        "# Dimensions of resulting tensor\n",
        "np.array(samples).shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGf2MHB6vCcM"
      },
      "source": [
        "### Reshape\n",
        "\n",
        "This is necessary so that the dimensions of the tensor are in the correct order, that is, to change the shape from (samples, features, timesteps) to (samples, timesteps, features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzFDbedY-wA9"
      },
      "outputs": [],
      "source": [
        "#Reshape\n",
        "reshaped_s = np.array(samples).reshape(-1,nSamplesInEach,nFeatures)\n",
        "np.array(reshaped_s).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaNfAMNcn-qx"
      },
      "outputs": [],
      "source": [
        "# Length of datasrt after preprocessing\n",
        "print(len(reshaped_s))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding\n",
        "\n",
        "To make training easier, the labels are encoded to binary values, which are easier to process and map in a neural network."
      ],
      "metadata": {
        "id": "SUWPHVKCvYx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Labels\n",
        "text_labels = labels[:]\n",
        "orig_labels = list(dict.fromkeys(labels)) # Get unique labels\n",
        "for i in range(len(text_labels)):\n",
        "  if text_labels[i] == \"Extension\":\n",
        "    text_labels[i] = 0\n",
        "  elif text_labels[i] == \"Flexion\":\n",
        "    text_labels[i] = 1\n",
        "  elif text_labels[i] == \"Pronacion\":\n",
        "    text_labels[i] = 2\n",
        "  else:\n",
        "    text_labels[i] = 3\n",
        "text_labels = np.array(text_labels)\n",
        "print(orig_labels)\n",
        "print(text_labels)"
      ],
      "metadata": {
        "id": "Ze4AqFLn_D_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(pd.get_dummies(labels))\n",
        "labels[:10]"
      ],
      "metadata": {
        "id": "7XZQ_MYKAX4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify unique encoded labels\n",
        "_, idx = np.unique(labels, axis=0, return_index=True)\n",
        "encoded_labels = labels[np.sort(idx)]\n",
        "print(encoded_labels)"
      ],
      "metadata": {
        "id": "bZ25WYUhm8eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the label list\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "MRIHpOXZAc8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hekaGMStvwcW"
      },
      "source": [
        "## Creation of training, testing and validation splits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWMYJ8niAktf"
      },
      "outputs": [],
      "source": [
        "# Create a kfold object of 5 splits\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr5CV0x14L5h"
      },
      "source": [
        "## Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZxc32qm4L5x"
      },
      "source": [
        "### Architecture 1: LSTM(128) + Dropout + Fully Connected + Fully Connected\n",
        "\n",
        "Source: https://www.analyticsvidhya.com/blog/2021/07/implementing-lstm-for-human-activity-recognition-using-smartphone-accelerometer-data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ILh4iSY4L5y"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, Reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U-XU2D94L5y"
      },
      "source": [
        "This simple architecture uses a LSTM layer of 128 neurons to process the time series data, then a dropout layer to prevent overfitting, and two fully connected layers to map the information from previous layers to the 4 outputs.\n",
        "\n",
        "**Loss function:** Categorical Cross Entropy\n",
        "\n",
        "**Optimizer:** Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3EHmIZk4L5z"
      },
      "outputs": [],
      "source": [
        "def create_model(n_outputs):\n",
        "  model = Sequential()\n",
        "  # RNN layer\n",
        "  model.add(LSTM(units = 128, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
        "  # Dropout layer\n",
        "  model.add(Dropout(0.5))\n",
        "  # Dense layer with ReLu\n",
        "  model.add(Dense(units = 64, activation='relu'))\n",
        "  # Softmax layer\n",
        "  model.add(Dense(n_outputs, activation = 'softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVSjO_134L5z"
      },
      "source": [
        "#### Train the model\n",
        "\n",
        "Train the model for 50 epochs in mini-batches of 32 samples (because the dataset is small). This will be done for all the splits made by the kfold. This is 50 iterations over all samples in the `x_train` and `y_train` tensors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialization of train and test\n",
        "\n",
        "X = reshaped_s\n",
        "Y = labels"
      ],
      "metadata": {
        "id": "HhnsbX2Wn4Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 50, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch1_train_acc = np.mean(train_accuracies)\n",
        "arch1_train_loss = np.mean(train_losses)\n",
        "arch1_val_acc = np.mean(val_accuracies)\n",
        "arch1_val_loss = np.mean(val_losses)\n",
        "arch1_test_acc = np.mean(test_accuracies)\n",
        "arch1_test_loss = np.mean(test_losses)\n",
        "arch1_precision = np.mean(precisions)\n",
        "arch1_recall = np.mean(recalls)\n",
        "arch1_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch1_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch1_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch1_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch1_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch1_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch1_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch1_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch1_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch1_f1_score * 100, np.std(f1_scores) * 100))"
      ],
      "metadata": {
        "id": "IAibG35a_hV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbCYy-p4L51"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFyVDmj44L51"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Create 7 subplots in a grid with 3 rows and 3 columns\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(25, 10))\n",
        "\n",
        "axes[0, 0].plot(epochs, acc, 'r', label='Training Accuracy')\n",
        "axes[0, 0].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[0, 0].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[0, 0].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[0, 0].set_title(\"Architecture 1\")\n",
        "axes[0, 0].set_xlabel('Epochs')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHwmr4304L52"
      },
      "source": [
        "### Architecture 2: LSTM(128) + Dropout + LSTM(64) + Dropout + Fully Connected\n",
        "\n",
        "Source: https://github.com/srvds/Human-Activity-Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQjnRyRW4L52"
      },
      "source": [
        "This architecture uses a LSTM layer of 128 neurons to process the time series data, then a dropout layer to prevent overfitting, another LSTM and another dropout layer for improving the previous architecture, and a fully connected layer to map the information from previous layers to the 4 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFORBXn54L53"
      },
      "outputs": [],
      "source": [
        "def create_model(n_outputs):\n",
        "  # Initiliazing the sequential model\n",
        "  model = keras.Sequential()\n",
        "  # Add LSTM layer\n",
        "  model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "  # Adding a dropout layer\n",
        "  model.add(Dropout(0.5))\n",
        "  # Add another LSTM layer\n",
        "  model.add(LSTM(64))\n",
        "  # Adding a dropout layer\n",
        "  model.add(Dropout(0.5))\n",
        "  # Adding a dense output layer with sigmoid activation\n",
        "  model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  #lost function and optimizer\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZhRU_9W4L53"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YicuaXIC4L56"
      },
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, epochs = 50, validation_split=0.15, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch2_train_acc = np.mean(train_accuracies)\n",
        "arch2_train_loss = np.mean(train_losses)\n",
        "arch2_val_acc = np.mean(val_accuracies)\n",
        "arch2_val_loss = np.mean(val_losses)\n",
        "arch2_test_acc = np.mean(test_accuracies)\n",
        "arch2_test_loss = np.mean(test_losses)\n",
        "arch2_precision = np.mean(precisions)\n",
        "arch2_recall = np.mean(recalls)\n",
        "arch2_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch2_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch2_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch2_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch2_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch2_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch2_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch2_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch2_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch2_f1_score * 100, np.std(f1_scores) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cplpQg594L56"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PJJEGFc4L57"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[0, 1].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[0, 1].plot(epochs, vacc , 'm', label='Validation Accuracy')\n",
        "axes[0, 1].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[0, 1].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[0, 1].set_title(\"Architecture 2\")\n",
        "axes[0, 1].set_xlabel('Epochs')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGkJMjB14L57"
      },
      "source": [
        "### Architecture 3: Conv1D(64) + Conv1D(64) + Dropout + Max Pooling + Flatten + Fully Connected + Fully Connected\n",
        "\n",
        "Source: https://github.com/CDAC-lab/ETFA-Workshop/blob/main/CNN%20and%20LSTM%20for%20Human%20Activity%20Recognition.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsF3HxnO4L58"
      },
      "source": [
        "This architecture focuses on convolutional neural networks. The first two layers are 1D convolutions of 64 filters, then a dropout layer to prevent overfitting, a max pooling layer to reduce dimensionality, a flatten layer to use dense layers, and two fully connected layers to map the information from previous layers to the 4 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfo5LPyk4L58"
      },
      "outputs": [],
      "source": [
        "def create_model(n_outputs):\n",
        "\n",
        "  # Define the model\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # Add a 1D Convolutional layer with 32 filters and a kernel size of 3\n",
        "  model.add(Conv1D(filters=64, kernel_size=4, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "  # Add a 1D Convolutional layer with 64 filters and a kernel size of 3\n",
        "  model.add(Conv1D(filters=64, kernel_size=4, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "\n",
        "  # Add a dropout layer with a rate of 0.5\n",
        "  model.add(Dropout(rate=0.5))\n",
        "\n",
        "  # Add a Max Pooling layer with a pool size of 2\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  # Add a Flatten layer\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Add a fully connected layer\n",
        "  model.add(Dense(units=100, activation='relu'))\n",
        "\n",
        "  # Add an output layer\n",
        "  model.add(Dense(units=n_outputs, activation='softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA_AC06v4L59"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YHqjkma4L59"
      },
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 50, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch3_train_acc = np.mean(train_accuracies)\n",
        "arch3_train_loss = np.mean(train_losses)\n",
        "arch3_val_acc = np.mean(val_accuracies)\n",
        "arch3_val_loss = np.mean(val_losses)\n",
        "arch3_test_acc = np.mean(test_accuracies)\n",
        "arch3_test_loss = np.mean(test_losses)\n",
        "arch3_precision = np.mean(precisions)\n",
        "arch3_recall = np.mean(recalls)\n",
        "arch3_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch3_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch3_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch3_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch3_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch3_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch3_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch3_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch3_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch3_f1_score * 100, np.std(f1_scores) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z7_yUFm4L5-"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Tu3y2X4L5-"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[0, 2].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[0, 2].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[0, 2].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[0, 2].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[0, 2].set_title(\"Architecture 3\")\n",
        "axes[0, 2].set_xlabel('Epochs')\n",
        "axes[0, 2].set_ylabel('Loss')\n",
        "axes[0, 2].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X12COR144L5_"
      },
      "source": [
        "### Architecture 4: Conv1D(32) + Max Pooling + LSTM(128) + Dropout + Fully Connected + Fully Connected\n",
        "\n",
        "Source: None, the network was done empyrically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPWToZ5n4L5_"
      },
      "source": [
        "This architecture uses a 1D convolutional layer of 32 filters, then a max pooling layer to reduce dimensionality, a LSTM layer of 128 units, a dropout layer to prevent overfitting, and two fully connected layers to map the information from previous layers to the 4 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUdgEpBn4L6A"
      },
      "outputs": [],
      "source": [
        "def create_model(n_outputs):\n",
        "\n",
        "  # Define the model\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # Add a 1D Convolutional layer with 32 filters and a kernel size of 3\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        "\n",
        "  # Add a Max Pooling layer with a pool size of 2\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  # Add an LSTM layer with 128 units\n",
        "  model.add(LSTM(units=128))\n",
        "\n",
        "  # Add a dropout layer with a rate of 0.5\n",
        "  model.add(Dropout(rate=0.5))\n",
        "\n",
        "  # Add a fully connected layer\n",
        "  model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "  # Add an output layer\n",
        "  model.add(Dense(units=n_outputs, activation='softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvK4bBq4L6A"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Y6kXTl4L6B"
      },
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 50, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch4_train_acc = np.mean(train_accuracies)\n",
        "arch4_train_loss = np.mean(train_losses)\n",
        "arch4_val_acc = np.mean(val_accuracies)\n",
        "arch4_val_loss = np.mean(val_losses)\n",
        "arch4_test_acc = np.mean(test_accuracies)\n",
        "arch4_test_loss = np.mean(test_losses)\n",
        "arch4_precision = np.mean(precisions)\n",
        "arch4_recall = np.mean(recalls)\n",
        "arch4_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch4_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch4_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch4_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch4_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch4_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch4_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch4_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch4_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch4_f1_score * 100, np.std(f1_scores) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5I6yo7R4L6C"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmgSA-j54L6D"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[0, 3].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[0, 3].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[0, 3].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[0, 3].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[0, 3].set_title(\"Architecture 4\")\n",
        "axes[0, 3].set_xlabel('Epochs')\n",
        "axes[0, 3].set_ylabel('Loss')\n",
        "axes[0, 3].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHolcJCi4L6E"
      },
      "source": [
        "### Architecture 5: Conv1D(32) + Conv1D(64) + Max Pooling + Bidirectional LSTM(128) + Dropout + Fully Connected + Fully Connected\n",
        "\n",
        "Source: One of the papers suggested using bidirectional LSTM as an improvement for HAR, the added convolutional layers are for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv_gYmKm4L6H"
      },
      "source": [
        "This architecture uses two layers of 1D convolutions, one using 32 filters and the other using 64 filters, then a max pooling layer to reduce dimensionality, a bidirectional LSTM of 128 neurons, a dropout layer to prevent overfitting, and two fully connected layers to map the information from previous layers to the 4 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bw1SIve4L6I"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional, Attention\n",
        "\n",
        "def create_model(n_outputs):\n",
        "\n",
        "  # Define the model\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # Add a 1D Convolutional layer with 32 filters and a kernel size of 3\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        "\n",
        "  # Add a second 1D Convolutional layer with 64 filters and a kernel size of 3\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "\n",
        "  # Add a Max Pooling layer with a pool size of 2\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "  # Add a bidirectional LSTM layer with 128 units\n",
        "  model.add(Bidirectional(LSTM(units=128)))\n",
        "\n",
        "  # Add a dropout layer with a rate of 0.5\n",
        "  model.add(Dropout(rate=0.5))\n",
        "\n",
        "  # Add a fully connected layer\n",
        "  model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "  # Add an output layer\n",
        "  model.add(Dense(units=n_outputs, activation='softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4_eUrJ4L6J"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P96k8LuF4L6J"
      },
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 50, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch5_train_acc = np.mean(train_accuracies)\n",
        "arch5_train_loss = np.mean(train_losses)\n",
        "arch5_val_acc = np.mean(val_accuracies)\n",
        "arch5_val_loss = np.mean(val_losses)\n",
        "arch5_test_acc = np.mean(test_accuracies)\n",
        "arch5_test_loss = np.mean(test_losses)\n",
        "arch5_precision = np.mean(precisions)\n",
        "arch5_recall = np.mean(recalls)\n",
        "arch5_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch5_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch5_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch5_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch5_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch5_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch5_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch5_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch5_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch5_f1_score * 100, np.std(f1_scores) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdgRgoj34L6L"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQpaWVhq4L6M"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[1, 0].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[1, 0].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[1, 0].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[1, 0].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[1, 0].set_title(\"Architecture 5\")\n",
        "axes[1, 0].set_xlabel('Epochs')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny7JG3F_4L6M"
      },
      "source": [
        "### Architecture 6: LSTM(128) + Dropout + Reshape + Conv1D(32) + Dropout + Flatten + Fully Connected + Fully Connected\n",
        "\n",
        "Source: Empyrical adaptation of the architechture of the model with convolutional layers at the start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUcrFSyZ4L6N"
      },
      "source": [
        "The sixth architecture starts with a LSTM layer, followed by a dropout layer to avoid overfitting and a reshape layer in order to use a convolutional layer. The next layer is a 1D convolution with 32 filters, followed by another dropout layer, and a flatter layer in order to use dense layers. The last two layers are fully connected layers in order to map the information from previous layers to the 4 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5QxttbU4L6O"
      },
      "outputs": [],
      "source": [
        "def create_model(n_outputs):\n",
        "\n",
        "  # Define the model\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # Add an LSTM layer with 128 units\n",
        "  model.add(LSTM(units=128, input_shape=(n_timesteps, n_features)))\n",
        "\n",
        "  # Add a dropout layer with a rate of 0.5\n",
        "  model.add(Dropout(rate=0.5))\n",
        "\n",
        "  # Reshape the input data to the correct shape for the Conv1D layer\n",
        "  model.add(Reshape((-1, 128)))\n",
        "\n",
        "  # Add a 1D Convolutional layer with 32 filters and a kernel size of 3\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal'))\n",
        "\n",
        "  # Add a dropout layer with a rate of 0.5\n",
        "  model.add(Dropout(rate=0.5))\n",
        "\n",
        "  # Add a Flatten layer\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Add a fully connected layer\n",
        "  model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "  # Add an output layer\n",
        "  model.add(Dense(units=n_outputs, activation='softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LeCHj104L6O"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1eqESQf4L6P"
      },
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 50, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch6_train_acc = np.mean(train_accuracies)\n",
        "arch6_train_loss = np.mean(train_losses)\n",
        "arch6_val_acc = np.mean(val_accuracies)\n",
        "arch6_val_loss = np.mean(val_losses)\n",
        "arch6_test_acc = np.mean(test_accuracies)\n",
        "arch6_test_loss = np.mean(test_losses)\n",
        "arch6_precision = np.mean(precisions)\n",
        "arch6_recall = np.mean(recalls)\n",
        "arch6_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch6_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch6_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch6_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch6_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch6_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch6_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch6_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch6_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch6_f1_score * 100, np.std(f1_scores) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ4F9ujy4L6Q"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMG6VLU44L6R"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[1, 1].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[1, 1].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[1, 1].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[1, 1].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[1, 1].set_title(\"Architecture 6\")\n",
        "axes[1, 1].set_xlabel('Epochs')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddxsQEH4L6S"
      },
      "source": [
        "### Architecture 7: LSTM(64) + Dropout + Fully Connected + Fully Connected + Fully Connected\n",
        "\n",
        "Source: Empyrical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAY9U3EH4L6S"
      },
      "source": [
        "The final architecture is a very simple one, starting with a LSTM layer, followed by a dropout layer to avoid overfitting and three fully connected layers in order to map the information from previous layers to the 4 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP0UhE744L6T"
      },
      "outputs": [],
      "source": [
        "def create_model(n_outputs):\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(LSTM(64,input_shape=(n_timesteps,n_features)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA8WDyfQ4L6U"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35hu94sw4L6U"
      },
      "outputs": [],
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], Y_train.shape[1]\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.15, epochs = 50, batch_size = 32, verbose = 0)\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=1)\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      index = int(np.where(encoding == 1)[0])\n",
        "      Y_test_flat.append(index)\n",
        "\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = accuracy_score(Y_test, Y_pred)\n",
        "    test_loss = log_loss(Y_test, model.predict(X_test))\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch7_train_acc = np.mean(train_accuracies)\n",
        "arch7_train_loss = np.mean(train_losses)\n",
        "arch7_val_acc = np.mean(val_accuracies)\n",
        "arch7_val_loss = np.mean(val_losses)\n",
        "arch7_test_acc = np.mean(test_accuracies)\n",
        "arch7_test_loss = np.mean(test_losses)\n",
        "arch7_precision = np.mean(precisions)\n",
        "arch7_recall = np.mean(recalls)\n",
        "arch7_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch7_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch7_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch7_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch7_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch7_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch7_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch7_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch7_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch7_f1_score * 100, np.std(f1_scores) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCaOJbfc4L6V"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYY-PioV4L6W"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[1, 2].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[1, 2].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[1, 2].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[1, 2].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[1, 2].set_title(\"Architecture 7\")\n",
        "axes[1, 2].set_xlabel('Epochs')\n",
        "axes[1, 2].set_ylabel('Loss')\n",
        "axes[1, 2].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qQbt-iXilA"
      },
      "source": [
        "### Architecture 8: Normalization + Transformer + Normalization + Dense\n",
        "\n",
        "Source: https://www.mdpi.com/1424-8220/22/5/1911"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JstpmMWX6c8"
      },
      "outputs": [],
      "source": [
        "!pip3 install git+https://github.com/tensorflow/addons.git\n",
        "\n",
        "from keras.layers import  Add, MultiHeadAttention, LayerNormalization, Layer, Normalization\n",
        "import math\n",
        "from keras import Model\n",
        "from keras.initializers import TruncatedNormal\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, LearningRateScheduler, Callback\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFGhNtyZznL"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(Layer):\n",
        "    def __init__(self, units, dropout_rate, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
        "\n",
        "        self.dropout = Dropout(rate=dropout_rate)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(PositionalEmbedding, self).build(input_shape)\n",
        "\n",
        "        self.position = self.add_weight(\n",
        "            name=\"position\",\n",
        "            shape=(1, input_shape[1], self.units),\n",
        "            initializer=TruncatedNormal(stddev=0.02),\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.projection(inputs)\n",
        "        x = x + self.position\n",
        "\n",
        "        return self.dropout(x, training=training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIwd6GlIZznM"
      },
      "outputs": [],
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(\n",
        "        self, embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate, **kwargs\n",
        "    ):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "\n",
        "        self.mha = MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim,\n",
        "            dropout=attention_dropout_rate,\n",
        "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
        "        )\n",
        "\n",
        "        self.dense_0 = Dense(\n",
        "            units=mlp_dim,\n",
        "            activation=\"gelu\",\n",
        "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
        "        )\n",
        "        self.dense_1 = Dense(\n",
        "            units=embed_dim, kernel_initializer=TruncatedNormal(stddev=0.02)\n",
        "        )\n",
        "\n",
        "        self.dropout_0 = Dropout(rate=dropout_rate)\n",
        "        self.dropout_1 = Dropout(rate=dropout_rate)\n",
        "\n",
        "        self.norm_0 = LayerNormalization(epsilon=1e-5)\n",
        "        self.norm_1 = LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "        self.add_0 = Add()\n",
        "        self.add_1 = Add()\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Attention block\n",
        "        x = self.norm_0(inputs)\n",
        "        x = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            training=training,\n",
        "        )\n",
        "        x = self.dropout_0(x, training=training)\n",
        "        x = self.add_0([x, inputs])\n",
        "\n",
        "        # MLP block\n",
        "        y = self.norm_1(x)\n",
        "        y = self.dense_0(y)\n",
        "        y = self.dense_1(y)\n",
        "        y = self.dropout_1(y, training=training)\n",
        "\n",
        "        return self.add_1([x, y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYEKK7pYZznN"
      },
      "outputs": [],
      "source": [
        "class Transformer(Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        embed_dim,\n",
        "        mlp_dim,\n",
        "        num_heads,\n",
        "        num_classes,\n",
        "        dropout_rate,\n",
        "        attention_dropout_rate,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(Transformer, self).__init__(**kwargs)\n",
        "\n",
        "        # Input (normalization of RAW measurements)\n",
        "        self.input_norm = Normalization()\n",
        "\n",
        "        # Input\n",
        "        self.pos_embs = PositionalEmbedding(embed_dim, dropout_rate)\n",
        "\n",
        "        # Encoder\n",
        "        self.e_layers = [\n",
        "            Encoder(embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        # Output\n",
        "        self.norm = LayerNormalization(epsilon=1e-5)\n",
        "        self.final_layer = Dense(num_classes, kernel_initializer=\"zeros\")\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.input_norm(inputs)\n",
        "        x = self.pos_embs(x, training=training)\n",
        "\n",
        "        for layer in self.e_layers:\n",
        "            x = layer(x, training=training)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.final_layer(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjP4eLOkLZeC"
      },
      "source": [
        "#### Loss function and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK6QapYViAIb"
      },
      "outputs": [],
      "source": [
        "#Loss\n",
        "def smoothed_sparse_categorical_crossentropy(label_smoothing: float = 0.0):\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        num_classes = tf.shape(y_pred)[-1]\n",
        "        y_true = tf.one_hot(y_true, num_classes)\n",
        "\n",
        "        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True, label_smoothing=label_smoothing)\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_fn\n",
        "\n",
        "def cosine_schedule(base_lr, total_steps, warmup_steps):\n",
        "    def step_fn(epoch):\n",
        "        lr = base_lr\n",
        "        epoch += 1\n",
        "\n",
        "        progress = (epoch - warmup_steps) / float(total_steps - warmup_steps)\n",
        "        progress = tf.clip_by_value(progress, 0.0, 1.0)\n",
        "\n",
        "        lr = lr * 0.5 * (1.0 + tf.cos(math.pi * progress))\n",
        "\n",
        "        if warmup_steps:\n",
        "            lr = lr * tf.minimum(1.0, epoch / warmup_steps)\n",
        "\n",
        "        return lr\n",
        "\n",
        "    return step_fn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_labels = np.repeat(text_labels.reshape(text_labels.shape[0], 1), reshaped_s.shape[1], axis=1)"
      ],
      "metadata": {
        "id": "fX7fPrj9iyFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaZxGMgKZznS"
      },
      "outputs": [],
      "source": [
        "amsgrad = False\n",
        "attention_dropout = 0.1\n",
        "batch_size = 32\n",
        "dropout = 0.1\n",
        "embed_layer_size = 128\n",
        "epochs = 50\n",
        "fc_layer_size = 256\n",
        "global_clipnorm = 3\n",
        "label_smoothing = 0.05\n",
        "learning_rate = 0.001\n",
        "num_heads = 8\n",
        "num_layers = 3\n",
        "optimizer = \"adam\"\n",
        "warmup_steps = 10\n",
        "\n",
        "def create_model(n_outputs):\n",
        "\n",
        "  # Generate new model\n",
        "  model = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    embed_dim=embed_layer_size,\n",
        "    mlp_dim=fc_layer_size,\n",
        "    num_heads=num_heads,\n",
        "    num_classes=n_outputs,\n",
        "    dropout_rate=dropout,\n",
        "    attention_dropout_rate=attention_dropout,\n",
        "  )\n",
        "\n",
        "  # adapt on training dataset - must be before model.compile !!!\n",
        "  model.input_norm.adapt(X_train, batch_size=batch_size)\n",
        "\n",
        "  # Select optimizer\n",
        "  if optimizer == \"adam\":\n",
        "    optim = Adam(\n",
        "        global_clipnorm=global_clipnorm,\n",
        "        amsgrad=amsgrad,\n",
        "    )\n",
        "\n",
        "  model.compile(\n",
        "    loss=smoothed_sparse_categorical_crossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=optim,\n",
        "    metrics=[\"accuracy\"],\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "test_accuracies = []\n",
        "test_losses = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "Y = text_labels\n",
        "\n",
        "for id, (train_index, test_index) in enumerate(kfold.split(X)):\n",
        "\n",
        "    print(f\"------------------------------------------ SPLIT {id + 1} -------------------------------------------\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    print(f\"X train length: {len(X_train)}    X test length: {len(X_test)}\")\n",
        "    print(f\"y train length: {len(Y_train)}    y test length: {len(Y_test)}\")\n",
        "\n",
        "    # Get the input shape -> (samples, time steps, features)\n",
        "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], n_outputs\n",
        "    print('n_timesteps: {}, n_features: {}, n_outputs: {}'.format(n_timesteps,n_features, n_outputs))\n",
        "\n",
        "    model = create_model(n_outputs)\n",
        "\n",
        "    history = model.fit(\n",
        "      X_train,\n",
        "      Y_train,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_split=0.15,\n",
        "      callbacks=[\n",
        "        LearningRateScheduler(cosine_schedule(base_lr=learning_rate, total_steps=epochs, warmup_steps=warmup_steps))\n",
        "      ],\n",
        "      verbose=0\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    results = model.evaluate(X_test, Y_test, batch_size = 32, verbose = 1)\n",
        "\n",
        "    history_dict = history.history\n",
        "\n",
        "    # Get the historical values for accuracies and losses\n",
        "\n",
        "    acc = history_dict['accuracy']\n",
        "    vacc = history_dict['val_accuracy']\n",
        "    loss = history_dict['loss']\n",
        "    vloss = history_dict['val_loss']\n",
        "\n",
        "    # Save the last values for accuracies and losses for comparison\n",
        "\n",
        "    train_acc = acc[-1]\n",
        "    val_acc = vacc[-1]\n",
        "    train_loss = loss[-1]\n",
        "    val_loss = vloss[-1]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred_aux = model.predict(X_test)\n",
        "    Y_pred = tf.argmax(Y_pred, axis=2)\n",
        "    Y_pred = Y_pred.numpy()\n",
        "    Y_pred_lcalc = [] # For computing loss\n",
        "    Y_pred_flat = []\n",
        "    Y_test_flat = []\n",
        "\n",
        "    for encoding in Y_pred:\n",
        "      yp = encoding[0] # Extract the Y_pred label\n",
        "      Y_pred_flat.append(yp)\n",
        "\n",
        "    for encoding in Y_test:\n",
        "      yt = encoding[0] # Extract the Y_test label\n",
        "      Y_test_flat.append(yt)\n",
        "\n",
        "    Y_pred = tf.constant(Y_pred_flat)\n",
        "    Y_test = tf.constant(Y_test_flat)\n",
        "\n",
        "    test_acc = results[1]\n",
        "    test_loss = results[0]\n",
        "    precision = precision_score(Y_test, Y_pred, average = 'weighted')\n",
        "    recall = recall_score(Y_test, Y_pred, average = 'weighted')\n",
        "    f1score = f1_score(Y_test, Y_pred, average = 'weighted')\n",
        "\n",
        "    test_accuracies.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1score)\n",
        "\n",
        "    print(\"Train Accuracy: \", train_acc)\n",
        "    print(\"Train Loss: \", train_loss)\n",
        "    print(\"Validation Accuracy: \", val_acc)\n",
        "    print(\"Validation Loss: \", val_loss)\n",
        "    print(\"Test Accuracy: \", test_acc)\n",
        "    print(\"Test Loss: \", test_loss)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1score)\n",
        "    print(f\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "arch8_train_acc = np.mean(train_accuracies)\n",
        "arch8_train_loss = np.mean(train_losses)\n",
        "arch8_val_acc = np.mean(val_accuracies)\n",
        "arch8_val_loss = np.mean(val_losses)\n",
        "arch8_test_acc = np.mean(test_accuracies)\n",
        "arch8_test_loss = np.mean(test_losses)\n",
        "arch8_precision = np.mean(precisions)\n",
        "arch8_recall = np.mean(recalls)\n",
        "arch8_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(\"Mean Train Accuracy: %.2f%% (+/- %.2f%%)\" % (arch8_train_acc * 100, np.std(train_accuracies) * 100))\n",
        "print(\"Mean Train Loss: %.2f%%\" % (arch8_train_loss * 100))\n",
        "print(\"Mean Validation Accuracy: %.2f%% (+/- %.2f%%)\" % (arch8_val_acc * 100, np.std(val_accuracies) * 100))\n",
        "print(\"Mean Validation Loss: %.2f%%\" % (arch8_val_loss * 100))\n",
        "print(\"Mean Test Accuracy: %.2f%% (+/- %.2f%%)\" % (arch8_test_acc * 100, np.std(test_accuracies) * 100))\n",
        "print(\"Mean Test Loss: %.2f%%\" % (arch8_test_loss * 100))\n",
        "print(\"Mean Precision: %.2f%% (+/- %.2f%%)\" % (arch8_precision * 100, np.std(precisions) * 100))\n",
        "print(\"Mean Recall: %.2f%% (+/- %.2f%%)\" % (arch8_recall * 100, np.std(recalls) * 100))\n",
        "print(\"Mean F1 score: %.2f%% (+/- %.2f%%)\" % (arch8_f1_score * 100, np.std(f1_scores) * 100))"
      ],
      "metadata": {
        "id": "xKfM85PvayIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2RP3L4_mhMG"
      },
      "source": [
        "#### Create a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkvlh_GKmhMH"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "axes[1, 3].plot(epochs, acc , 'r', label='Training Accuracy')\n",
        "axes[1, 3].plot(epochs, vacc, 'm', label='Validation Accuracy')\n",
        "axes[1, 3].plot(epochs, loss, 'g', label='Training loss')\n",
        "axes[1, 3].plot(epochs, vloss, 'b', label='Validation loss')\n",
        "axes[1, 3].set_title(\"Architecture 8\")\n",
        "axes[1, 3].set_xlabel('Epochs')\n",
        "axes[1, 3].set_ylabel('Loss')\n",
        "axes[1, 3].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dUYDsdhcTco"
      },
      "source": [
        "## Table Comparison of accuracies of the 8 architechtures\n",
        "\n",
        "After training and testing all the architectures and obtaining their accuracy and loss values, the obtained results are presented in a table to compare the performance of each architecture in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYKZiVe6cTcp"
      },
      "outputs": [],
      "source": [
        "# Architectures labels\n",
        "archs = [\"Architecture 1\", \"Architecture 2\", \"Architecture 3\", \"Architecture 4\", \"Architecture 5\", \"Architecture 6\", \"Architecture 7\", \"Architecture 8\"]\n",
        "# Training accuracies of the architectures\n",
        "train_accs = [arch1_train_acc, arch2_train_acc, arch3_train_acc, arch4_train_acc, arch5_train_acc, arch6_train_acc, arch7_train_acc, arch8_train_acc]\n",
        "# Training losses of the architectures\n",
        "train_losses = [arch1_test_loss, arch2_test_loss, arch3_test_loss, arch4_test_loss, arch5_test_loss, arch6_test_loss, arch7_train_loss, arch8_train_loss]\n",
        "# Testing accuracies of the architectures\n",
        "test_accs = [arch1_test_acc, arch2_test_acc, arch3_test_acc, arch4_test_acc, arch5_test_acc, arch6_test_acc, arch7_test_acc, arch8_test_acc]\n",
        "# Testing losses of the architectures\n",
        "test_losses = [arch1_test_loss, arch2_test_loss, arch3_test_loss, arch4_test_loss, arch5_test_loss, arch6_test_loss, arch7_test_loss, arch8_test_loss]\n",
        "# Precisions of the architectures\n",
        "precisions = [arch1_precision, arch2_precision, arch3_precision, arch4_precision, arch5_precision, arch6_precision, arch7_precision, arch8_precision]\n",
        "# Recalls of the architectures\n",
        "recalls = [arch1_recall, arch2_recall, arch3_recall, arch4_recall, arch5_recall, arch6_recall, arch7_recall, arch8_recall]\n",
        "# F1 scores of the architectures\n",
        "f1_scores = [arch1_f1_score, arch2_f1_score, arch3_f1_score, arch4_f1_score, arch5_f1_score, arch6_f1_score, arch7_f1_score, arch8_f1_score]\n",
        "\n",
        "df = pd.DataFrame(list(zip(archs, train_accs, train_losses, test_accs, test_losses, precisions, recalls, f1_scores)),\n",
        "                  columns =['Architectures', 'Training Accuracy', 'Training Loss', 'Test Accuracy', 'Test Loss', 'Precision', 'Recall', 'F1 score'])\n",
        "df\n",
        "\n",
        "# Defining custom function which returns\n",
        "# the list for df.style.apply() method\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['font-weight: %s' % 'bold' if cell else '' for cell in is_max]\n",
        "\n",
        "\n",
        "# Defining custom function which returns\n",
        "# the list for df.style.apply() method\n",
        "def highlight_min(s):\n",
        "    is_min = s == s.min()\n",
        "    return ['font-weight: %s' % 'bold' if cell else '' for cell in is_min]\n",
        "\n",
        "df.style.apply(highlight_max, subset = ['Training Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 score']).apply(highlight_min, subset = ['Training Loss', 'Test Loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J7I2gxZ8lCN"
      },
      "source": [
        "## Plot of the Performance of Every Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8buxDzlAYXaI"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "axes[1, 3].axis('off')\n",
        "display(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XuLf8VN8sKa"
      },
      "source": [
        "## Plot of Best Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "6-iqOFMVgjAG",
        "outputId": "f5c68a3e-31c9-4c82-b80a-3f4f6c17dd32"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x720 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best = 4\n",
        "\n",
        "if (best <= 4):\n",
        "  best_x = 0\n",
        "  best_y = best - 1\n",
        "else:\n",
        "  best_x = 1\n",
        "  best_y = (best - 1) - 4\n",
        "\n",
        "# Set the spacing between subplots to zero\n",
        "fig.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "# Hide the other subplots\n",
        "for i in range(2):\n",
        "    for j in range(4):\n",
        "        if i != best_x or j != best_y:\n",
        "            axes[i, j].set_visible(False)\n",
        "\n",
        "display(axes[best_x, best_y].figure)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}